# PROOFOUND CRITICAL GAPS IMPLEMENTATION GUIDE

**Document Version**: 1.0
**Purpose**: Step-by-step guides for implementing critical missing systems
**Audience**: Developers
**Last Updated**: 2025-10-30

---

## EXECUTIVE SUMMARY

This guide provides detailed, copy-paste-ready instructions for implementing the most critical missing systems identified in the codebase audit:

1. ‚úÖ **File Storage** (Supabase Storage) - HIGH PRIORITY
2. ‚úÖ **Analytics Tracking** - HIGH PRIORITY
3. ‚úÖ **Skills Taxonomy Expansion** - MEDIUM PRIORITY
4. ‚úÖ **Rate Limiting** - HIGH PRIORITY (Security)
5. ‚úÖ **Security Hardening** (CSP, Input Sanitization) - HIGH PRIORITY

**Estimated Total Time**: 3-5 days (1 developer)

---

## TABLE OF CONTENTS

1. [ROW-LEVEL SECURITY (RLS) IMPLEMENTATION](#1-row-level-security-rls-implementation)
2. [Analytics IP Anonymization](#2-analytics-ip-anonymization)
3. [File Storage Implementation](#3-file-storage-implementation)
4. [Analytics Tracking System](#4-analytics-tracking-system)
5. [Skills Taxonomy Expansion](#5-skills-taxonomy-expansion)
6. [Rate Limiting Implementation](#6-rate-limiting-implementation)
7. [Security Hardening](#7-security-hardening)
8. [Testing Checklists](#8-testing-checklists)

---

## 1. ROW-LEVEL SECURITY (RLS) IMPLEMENTATION

### 1.1 Overview

**Goal**: Protect all user data with Row-Level Security policies to prevent unauthorized access

**Status**: ‚úÖ **COMPLETED** - 124 RLS policies deployed across 20 existing tables (2025-10-30)

**Tech Stack**: Supabase RLS (PostgreSQL Row-Level Security)

**Time Estimate**: 2-3 days (initial deployment complete, 8 additional tables pending schema migration)

---

### 1.2 Why This Is Critical

**Without RLS, users can query other users' private data directly via Supabase client.**

Even with authentication, without RLS policies, any authenticated user could run:
```typescript
// ‚ùå WITHOUT RLS: This would return ALL users' profiles
const { data } = await supabase.from('profiles').select('*');

// ‚ùå WITHOUT RLS: User A could read User B's private messages
const { data } = await supabase
  .from('messages')
  .select('*')
  .eq('sender_id', 'user-b-id');
```

**With RLS enabled and policies deployed**, these queries are automatically filtered:
```typescript
// ‚úÖ WITH RLS: Returns only the authenticated user's own profile
const { data } = await supabase.from('profiles').select('*');
// Result: Only rows where auth.uid() = id

// ‚úÖ WITH RLS: User A cannot access User B's messages
const { data } = await supabase
  .from('messages')
  .select('*')
  .eq('sender_id', 'user-b-id');
// Result: Empty array (RLS blocks unauthorized access)
```

---

### 1.3 Step 1: Verify RLS Deployment Status

**Current Status** (as of 2025-10-30):

Run this SQL in Supabase SQL Editor to verify:

```sql
-- Check which tables have RLS enabled
SELECT schemaname, tablename, rowsecurity
FROM pg_tables
WHERE schemaname = 'public'
ORDER BY tablename;

-- Count total policies deployed
SELECT COUNT(*) as total_policies
FROM pg_policies
WHERE schemaname = 'public';

-- List policies by table
SELECT tablename, COUNT(*) as policy_count
FROM pg_policies
WHERE schemaname = 'public'
GROUP BY tablename
ORDER BY policy_count DESC;
```

**Expected Results**:
- ‚úÖ 20 tables with `rowsecurity = true`
- ‚úÖ 124 total policies
- ‚úÖ Average 6.2 policies per table

**Tables Protected** (20/20):
- profiles, individual_profiles, matching_profiles
- organizations, organization_members, organization_profiles
- skills, experiences, education, volunteering
- impact_stories, capabilities, evidence
- assignments, matches, match_interest
- skill_endorsements, growth_plans
- notifications, notification_preferences

**Tables Pending** (8 tables not yet created in schema):
- verification_requests, verification_responses, verification_appeals
- conversations, messages, blocked_users
- analytics_events
- content_reports, moderation_actions, user_violations

---

### 1.4 Step 2: Deploy RLS Policies for New Tables (When Created)

When the 8 pending tables are added to the schema, deploy these policies:

#### Verification Requests - CRITICAL PRIVACY

```sql
-- Enable RLS first
ALTER TABLE verification_requests ENABLE ROW LEVEL SECURITY;

-- Only requester sees verifier email (TIER 1 PII)
CREATE POLICY "only_requester_sees_verifier_email"
  ON verification_requests FOR SELECT
  USING (auth.uid() = profile_id);

-- Verifier can access via token (no auth required)
CREATE POLICY "verifier_access_via_token"
  ON verification_requests FOR SELECT
  USING (token = current_setting('request.jwt.claims')::json->>'verification_token');

-- Only requester can create verification requests
CREATE POLICY "users_can_create_own_verification_requests"
  ON verification_requests FOR INSERT
  WITH CHECK (profile_id = auth.uid());
```

#### Messages - Staged Privacy Protection

```sql
ALTER TABLE messages ENABLE ROW LEVEL SECURITY;

-- Users can only read messages in their own conversations
CREATE POLICY "users_read_messages_in_own_conversations"
  ON messages FOR SELECT
  USING (
    EXISTS (
      SELECT 1 FROM conversations
      WHERE conversations.id = messages.conversation_id
        AND (
          conversations.participant_one_id = auth.uid()
          OR conversations.participant_two_id = auth.uid()
        )
    )
  );

-- Users can only send messages to their own conversations
CREATE POLICY "users_send_messages_to_own_conversations"
  ON messages FOR INSERT
  WITH CHECK (
    EXISTS (
      SELECT 1 FROM conversations
      WHERE conversations.id = messages.conversation_id
        AND (
          conversations.participant_one_id = auth.uid()
          OR conversations.participant_two_id = auth.uid()
        )
    )
  );
```

#### Conversations - Participant-only Access

```sql
ALTER TABLE conversations ENABLE ROW LEVEL SECURITY;

-- Users can only see conversations they're part of
CREATE POLICY "users_read_own_conversations"
  ON conversations FOR SELECT
  USING (
    participant_one_id = auth.uid()
    OR participant_two_id = auth.uid()
  );

-- Users can update conversation stage (for identity reveal)
CREATE POLICY "users_can_update_conversation_stage"
  ON conversations FOR UPDATE
  USING (
    participant_one_id = auth.uid()
    OR participant_two_id = auth.uid()
  );
```

#### Analytics Events - Users See Own Data Only

```sql
ALTER TABLE analytics_events ENABLE ROW LEVEL SECURITY;

-- Users can only read their own analytics
CREATE POLICY "users_read_own_analytics"
  ON analytics_events FOR SELECT
  USING (user_id = auth.uid());

-- System can insert analytics (server-side only)
CREATE POLICY "service_role_can_insert_analytics"
  ON analytics_events FOR INSERT
  WITH CHECK (auth.jwt() ->> 'role' = 'service_role');
```

---

### 1.5 Step 3: Test RLS Policies

Create this test file to verify RLS policies work correctly:

**File**: `/tests/rls-policies.test.ts`

```typescript
import { createClient } from '@supabase/supabase-js';
import { describe, test, expect, beforeAll } from 'vitest';

describe('RLS Policy Tests', () => {
  let userAClient: any;
  let userBClient: any;
  let USER_A_ID: string;
  let USER_B_ID: string;

  beforeAll(async () => {
    // Create two test users
    userAClient = createClient(process.env.SUPABASE_URL!, process.env.SUPABASE_ANON_KEY!);
    userBClient = createClient(process.env.SUPABASE_URL!, process.env.SUPABASE_ANON_KEY!);

    // Sign in as User A
    const { data: userA } = await userAClient.auth.signInWithPassword({
      email: 'usera@test.com',
      password: 'password123'
    });
    USER_A_ID = userA.user.id;

    // Sign in as User B
    const { data: userB } = await userBClient.auth.signInWithPassword({
      email: 'userb@test.com',
      password: 'password123'
    });
    USER_B_ID = userB.user.id;
  });

  test('User A cannot read User B profile', async () => {
    // Attempt to read User B's profile as User A
    const { data, error } = await userAClient
      .from('profiles')
      .select('*')
      .eq('id', USER_B_ID)
      .single();

    // RLS should block this
    expect(data).toBeNull();
    expect(error).toBeDefined();
  });

  test('User can read their own profile', async () => {
    const { data, error } = await userAClient
      .from('profiles')
      .select('*')
      .eq('id', USER_A_ID)
      .single();

    expect(error).toBeNull();
    expect(data).toBeDefined();
    expect(data.id).toBe(USER_A_ID);
  });

  test('User A cannot read User B messages', async () => {
    // Attempt to read User B's messages
    const { data } = await userAClient
      .from('messages')
      .select('*')
      .eq('sender_id', USER_B_ID);

    // RLS should return empty array
    expect(data).toEqual([]);
  });

  test('Analytics events only show own data', async () => {
    const { data } = await userAClient
      .from('analytics_events')
      .select('*');

    // Should only return User A's events
    expect(data?.every(event => event.user_id === USER_A_ID)).toBe(true);
  });
});
```

**Run Tests**:
```bash
npm run test tests/rls-policies.test.ts
```

---

### 1.6 Step 4: Verify with Supabase Dashboard

1. Go to **Supabase Dashboard** ‚Üí **Authentication** ‚Üí **Policies**
2. Verify all tables show **"RLS Enabled"** badge
3. Check each table has 2-4 policies listed
4. Test with **"RLS Playground"**:
   - Select a table (e.g., `profiles`)
   - Choose a test user
   - Run query: `SELECT * FROM profiles`
   - Verify: Only shows that user's own profile

---

### 1.7 Testing Checklist

- [ ] All 20 existing tables have RLS enabled
- [ ] User A cannot query User B's profile data
- [ ] User A cannot read User B's messages
- [ ] Verifier email hidden from public queries (when table exists)
- [ ] Analytics events filtered to authenticated user only
- [ ] Service role can insert analytics (server-side)
- [ ] RLS Playground tests pass for all tables

---

## 2. ANALYTICS IP ANONYMIZATION

### 2.1 Overview

**Goal**: Hash IP addresses before storage to comply with GDPR Article 4(1)

**Current Issue**: `analyticsEvents` table stores raw IP addresses (PII under GDPR)

**Time Estimate**: 1 day

---

### 2.2 Why This Is Critical

**GDPR Article 4(1)** defines IP addresses as personal data. Storing raw IPs without explicit consent and retention policy is a **GDPR violation**.

**Current Schema** (‚ùå VIOLATES GDPR):
```typescript
export const analyticsEvents = pgTable('analytics_events', {
  ipAddress: text('ip_address'), // üî¥ RAW IP = PII under GDPR
  userAgent: text('user_agent'),
  sessionId: text('session_id'),
});
```

**Required Fix** (‚úÖ GDPR COMPLIANT):
```typescript
export const analyticsEvents = pgTable('analytics_events', {
  ipHash: text('ip_hash'), // ‚úÖ SHA-256 hash of IP
  userAgentHash: text('user_agent_hash'), // ‚úÖ Hashed
  // Raw IPs never stored
});
```

---

### 2.3 Step 1: Update Schema

**File**: `/src/db/schema.ts`

```typescript
export const analyticsEvents = pgTable('analytics_events', {
  id: uuid('id').defaultRandom().primaryKey(),
  eventType: text('event_type').notNull(),
  userId: uuid('user_id').references(() => profiles.id, { onDelete: 'cascade' }),
  orgId: uuid('org_id').references(() => organizations.id, { onDelete: 'cascade' }),
  properties: jsonb('properties').default(sql`'{}'::jsonb`),
  sessionId: text('session_id'),
  ipHash: text('ip_hash'), // ‚úÖ CHANGED: Store hash, not raw IP
  userAgentHash: text('user_agent_hash'), // ‚úÖ CHANGED: Hash user agent too
  createdAt: timestamp('created_at').defaultNow().notNull(),
});
```

**Generate Migration**:
```bash
npm run db:generate -- --name anonymize_analytics_pii
```

**Apply Migration**:
```bash
npm run db:push
```

---

### 2.4 Step 2: Create Hashing Utility

**File**: `/src/lib/utils/privacy.ts` (create new file)

```typescript
import crypto from 'crypto';

/**
 * Hash PII for analytics storage (one-way, irreversible)
 * Complies with GDPR Article 4(5) - pseudonymization
 */
export function hashPII(value: string, salt?: string): string {
  if (!value) return '';

  const hashSalt = salt || process.env.PII_HASH_SALT;
  
  if (!hashSalt) {
    throw new Error('PII_HASH_SALT environment variable not set');
  }

  return crypto
    .createHash('sha256')
    .update(value + hashSalt)
    .digest('hex');
}

/**
 * Anonymize IP address for analytics
 * @param ip - Raw IP address (IPv4 or IPv6)
 * @returns SHA-256 hash of IP (irreversible)
 */
export function anonymizeIP(ip: string): string {
  return hashPII(ip);
}

/**
 * Anonymize User Agent string
 */
export function anonymizeUserAgent(ua: string): string {
  return hashPII(ua);
}
```

---

### 2.5 Step 3: Update Analytics Tracking

**File**: `/src/lib/analytics.ts` (update existing or create)

```typescript
import { anonymizeIP, anonymizeUserAgent } from '@/lib/utils/privacy';
import { createClient } from '@/lib/supabase/server';

/**
 * Track analytics event with privacy-preserving hashing
 */
export async function trackEvent(
  eventType: string,
  properties: Record<string, any> = {},
  userId?: string,
  request?: Request
) {
  try {
    const supabase = await createClient();

    // Get user if not provided
    if (!userId) {
      const { data: { user } } = await supabase.auth.getUser();
      userId = user?.id;
    }

    // Extract IP and User Agent from request (if available)
    let ipHash: string | null = null;
    let userAgentHash: string | null = null;

    if (request) {
      const ip = request.headers.get('x-forwarded-for') ||
                 request.headers.get('x-real-ip') ||
                 'unknown';
      const userAgent = request.headers.get('user-agent') || 'unknown';

      // Hash before storage (GDPR compliant)
      ipHash = anonymizeIP(ip);
      userAgentHash = anonymizeUserAgent(userAgent);
    }

    // Insert event with hashed PII
    await supabase.from('analytics_events').insert({
      event_type: eventType,
      user_id: userId,
      properties,
      ip_hash: ipHash, // ‚úÖ Hashed, not raw
      user_agent_hash: userAgentHash, // ‚úÖ Hashed, not raw
      created_at: new Date().toISOString(),
    });

    // Log to console in development
    if (process.env.NODE_ENV === 'development') {
      console.log(`üìä [Analytics] ${eventType}`, { userId, properties });
    }
  } catch (error) {
    // Don't throw - analytics failures shouldn't break app
    console.error('Failed to track event:', error);
  }
}
```

---

### 2.6 Step 4: Add Environment Variable

**File**: `.env.local` (add)

```bash
# Generate with: openssl rand -hex 32
PII_HASH_SALT=<your-64-character-hex-salt>
```

**Generate Salt**:
```bash
openssl rand -hex 32
```

**Example Output**:
```
a3c8f9e2b7d4c1a8f6e9d2b5c8a3f6e9d2b5c8a3f6e9d2b5c8a3f6e9d2b5c8a3
```

**‚ö†Ô∏è SECURITY NOTE**: 
- NEVER commit `PII_HASH_SALT` to git
- Store in Vercel environment variables for production
- Use different salts for staging vs production

**Add to Vercel**:
```bash
vercel env add PII_HASH_SALT
# Paste the generated salt when prompted
```

---

### 2.7 Step 5: Update All Tracking Calls

Find all places where analytics are tracked and update them:

**Example - Auth Action**:

**File**: `/src/actions/auth.ts`

```typescript
import { trackEvent } from '@/lib/analytics';

export async function signUpWithEmail(email: string, password: string, request: Request) {
  // ... existing signup logic ...

  // Track signup event with request for IP hashing
  await trackEvent('signed_up', { method: 'email' }, user.id, request);
}
```

**Example - API Route**:

**File**: `/src/app/api/matches/accept/route.ts`

```typescript
export async function POST(request: NextRequest) {
  // ... existing logic ...

  // Track match acceptance with request
  await trackEvent('match_accepted', { 
    matchId, 
    score 
  }, user.id, request);
}
```

---

### 2.8 Testing Checklist

- [ ] Schema updated with `ip_hash` and `user_agent_hash`
- [ ] Migration applied successfully
- [ ] `hashPII()` utility function created
- [ ] `PII_HASH_SALT` added to all environments
- [ ] `trackEvent()` function updated to use hashing
- [ ] All tracking calls updated to pass `request` parameter
- [ ] Test: Insert analytics event ‚Üí verify IP is hashed (64-char hex)
- [ ] Test: Query `analytics_events` ‚Üí confirm no raw IPs stored
- [ ] Verify salt is NOT in git (check `.gitignore`)

**Test Query**:
```sql
-- Verify no raw IPs in database
SELECT ip_hash, LENGTH(ip_hash) as hash_length
FROM analytics_events
LIMIT 10;

-- Expected: hash_length = 64 (SHA-256 produces 64-char hex)
-- Expected: ip_hash looks like: "a3c8f9e2b7d4c1a8f6e9d2b5c8a3f6e9..."
```

---

## 3. FILE STORAGE IMPLEMENTATION

### 3.1 Overview

**Goal**: Enable users to upload proof evidence (PDFs, images) up to 5MB

**Tech Stack**: Supabase Storage

**Time Estimate**: 1 day

---

### 3.2 Step 1: Configure Supabase Storage (30 minutes)

#### Via Supabase Dashboard

1. Go to https://supabase.com/dashboard
2. Select your project
3. Navigate to **Storage** in left sidebar
4. Click **Create Bucket**

**Create 4 Buckets**:

| Bucket Name | Public | Max Size | Allowed Types |
|-------------|--------|----------|---------------|
| `proofs` | ‚ùå No | 5 MB | `application/pdf`, `image/*` |
| `avatars` | ‚úÖ Yes | 2 MB | `image/jpeg`, `image/png`, `image/webp` |
| `covers` | ‚úÖ Yes | 5 MB | `image/*` |
| `logos` | ‚úÖ Yes | 2 MB | `image/png`, `image/svg+xml` |

#### Via SQL (Alternative)

```sql
-- Run in Supabase SQL Editor

-- Create buckets (if not created via dashboard)
INSERT INTO storage.buckets (id, name, public)
VALUES
  ('proofs', 'proofs', false),
  ('avatars', 'avatars', true),
  ('covers', 'covers', true),
  ('logos', 'logos', true);

-- Set size limits (5MB = 5242880 bytes)
UPDATE storage.buckets
SET file_size_limit = 5242880
WHERE id IN ('proofs', 'covers');

UPDATE storage.buckets
SET file_size_limit = 2097152
WHERE id IN ('avatars', 'logos');
```

---

### 3.3 Step 2: Configure RLS Policies (15 minutes)

```sql
-- Run in Supabase SQL Editor

-- =============================================================================
-- PROOFS BUCKET (Private - only owner can read/write)
-- =============================================================================

-- Users can upload their own proofs
CREATE POLICY "Users can upload own proofs"
  ON storage.objects
  FOR INSERT
  WITH CHECK (
    bucket_id = 'proofs'
    AND auth.uid() = owner
  );

-- Users can read their own proofs
CREATE POLICY "Users can read own proofs"
  ON storage.objects
  FOR SELECT
  USING (
    bucket_id = 'proofs'
    AND auth.uid() = owner
  );

-- Users can update their own proofs
CREATE POLICY "Users can update own proofs"
  ON storage.objects
  FOR UPDATE
  USING (
    bucket_id = 'proofs'
    AND auth.uid() = owner
  );

-- Users can delete their own proofs
CREATE POLICY "Users can delete own proofs"
  ON storage.objects
  FOR DELETE
  USING (
    bucket_id = 'proofs'
    AND auth.uid() = owner
  );

-- =============================================================================
-- AVATARS BUCKET (Public read, authenticated write)
-- =============================================================================

-- Anyone can view avatars
CREATE POLICY "Anyone can view avatars"
  ON storage.objects
  FOR SELECT
  USING (bucket_id = 'avatars');

-- Users can upload their own avatar
CREATE POLICY "Users can upload own avatar"
  ON storage.objects
  FOR INSERT
  WITH CHECK (
    bucket_id = 'avatars'
    AND auth.uid() = owner
  );

-- Users can update their own avatar
CREATE POLICY "Users can update own avatar"
  ON storage.objects
  FOR UPDATE
  USING (
    bucket_id = 'avatars'
    AND auth.uid() = owner
  );

-- Users can delete their own avatar
CREATE POLICY "Users can delete own avatar"
  ON storage.objects
  FOR DELETE
  USING (
    bucket_id = 'avatars'
    AND auth.uid() = owner
  );

-- =============================================================================
-- COVERS & LOGOS BUCKETS (Same as avatars)
-- =============================================================================

-- Repeat for covers
CREATE POLICY "Anyone can view covers"
  ON storage.objects FOR SELECT USING (bucket_id = 'covers');

CREATE POLICY "Users can upload own cover"
  ON storage.objects FOR INSERT
  WITH CHECK (bucket_id = 'covers' AND auth.uid() = owner);

CREATE POLICY "Users can update own cover"
  ON storage.objects FOR UPDATE
  USING (bucket_id = 'covers' AND auth.uid() = owner);

CREATE POLICY "Users can delete own cover"
  ON storage.objects FOR DELETE
  USING (bucket_id = 'covers' AND auth.uid() = owner);

-- Repeat for logos
CREATE POLICY "Anyone can view logos"
  ON storage.objects FOR SELECT USING (bucket_id = 'logos');

CREATE POLICY "Users can upload own logo"
  ON storage.objects FOR INSERT
  WITH CHECK (bucket_id = 'logos' AND auth.uid() = owner);

CREATE POLICY "Users can update own logo"
  ON storage.objects FOR UPDATE
  USING (bucket_id = 'logos' AND auth.uid() = owner);

CREATE POLICY "Users can delete own logo"
  ON storage.objects FOR DELETE
  USING (bucket_id = 'logos' AND auth.uid() = owner);
```

---

### 3.4 Step 3: Create Storage Utility Library (1 hour)

**File**: `/src/lib/storage.ts`

```typescript
import { createClient } from '@/lib/supabase/client';
import { createClient as createServerClient } from '@/lib/supabase/server';

// =============================================================================
// TYPES
// =============================================================================

export type BucketName = 'proofs' | 'avatars' | 'covers' | 'logos';

export interface UploadOptions {
  bucket: BucketName;
  file: File;
  path?: string; // Optional custom path, otherwise auto-generated
  upsert?: boolean; // Overwrite if exists
}

export interface UploadResult {
  path: string;
  fullPath: string;
  publicUrl?: string; // Only for public buckets
}

// =============================================================================
// VALIDATION
// =============================================================================

const MAX_FILE_SIZES: Record<BucketName, number> = {
  proofs: 5 * 1024 * 1024, // 5MB
  avatars: 2 * 1024 * 1024, // 2MB
  covers: 5 * 1024 * 1024, // 5MB
  logos: 2 * 1024 * 1024, // 2MB
};

const ALLOWED_MIME_TYPES: Record<BucketName, string[]> = {
  proofs: ['application/pdf', 'image/jpeg', 'image/png', 'image/webp'],
  avatars: ['image/jpeg', 'image/png', 'image/webp'],
  covers: ['image/jpeg', 'image/png', 'image/webp'],
  logos: ['image/png', 'image/svg+xml', 'image/webp'],
};

export function validateFile(file: File, bucket: BucketName): { valid: boolean; error?: string } {
  // Check file size
  if (file.size > MAX_FILE_SIZES[bucket]) {
    return {
      valid: false,
      error: `File size exceeds ${MAX_FILE_SIZES[bucket] / 1024 / 1024}MB limit`,
    };
  }

  // Check MIME type
  if (!ALLOWED_MIME_TYPES[bucket].includes(file.type)) {
    return {
      valid: false,
      error: `File type ${file.type} not allowed for ${bucket}. Allowed: ${ALLOWED_MIME_TYPES[bucket].join(', ')}`,
    };
  }

  return { valid: true };
}

// =============================================================================
// UPLOAD (Client-side)
// =============================================================================

export async function uploadFile(options: UploadOptions): Promise<UploadResult> {
  const { bucket, file, path, upsert = false } = options;

  // Validate file
  const validation = validateFile(file, bucket);
  if (!validation.valid) {
    throw new Error(validation.error);
  }

  const supabase = createClient();

  // Get current user
  const {
    data: { user },
  } = await supabase.auth.getUser();
  if (!user) {
    throw new Error('User must be authenticated to upload files');
  }

  // Generate file path
  const fileName = path || generateFileName(file.name, user.id);
  const filePath = `${user.id}/${fileName}`;

  // Upload file
  const { data, error } = await supabase.storage
    .from(bucket)
    .upload(filePath, file, {
      cacheControl: '3600',
      upsert,
    });

  if (error) {
    console.error('Upload error:', error);
    throw new Error(`Failed to upload file: ${error.message}`);
  }

  // Get public URL (only for public buckets)
  let publicUrl: string | undefined;
  if (bucket !== 'proofs') {
    const { data: urlData } = supabase.storage.from(bucket).getPublicUrl(filePath);
    publicUrl = urlData.publicUrl;
  }

  return {
    path: data.path,
    fullPath: `${bucket}/${data.path}`,
    publicUrl,
  };
}

// =============================================================================
// DOWNLOAD / GET URL
// =============================================================================

export async function getSignedUrl(bucket: BucketName, path: string, expiresIn = 3600): Promise<string> {
  const supabase = createClient();

  const { data, error } = await supabase.storage.from(bucket).createSignedUrl(path, expiresIn);

  if (error) {
    throw new Error(`Failed to generate signed URL: ${error.message}`);
  }

  return data.signedUrl;
}

export async function getPublicUrl(bucket: BucketName, path: string): Promise<string> {
  if (bucket === 'proofs') {
    throw new Error('Proofs bucket is private. Use getSignedUrl instead.');
  }

  const supabase = createClient();
  const { data } = supabase.storage.from(bucket).getPublicUrl(path);

  return data.publicUrl;
}

// =============================================================================
// DELETE
// =============================================================================

export async function deleteFile(bucket: BucketName, path: string): Promise<void> {
  const supabase = createClient();

  const { error } = await supabase.storage.from(bucket).remove([path]);

  if (error) {
    throw new Error(`Failed to delete file: ${error.message}`);
  }
}

// =============================================================================
// LIST FILES
// =============================================================================

export async function listFiles(bucket: BucketName, folder?: string) {
  const supabase = createClient();

  const { data, error } = await supabase.storage.from(bucket).list(folder, {
    limit: 100,
    offset: 0,
    sortBy: { column: 'created_at', order: 'desc' },
  });

  if (error) {
    throw new Error(`Failed to list files: ${error.message}`);
  }

  return data;
}

// =============================================================================
// HELPERS
// =============================================================================

function generateFileName(originalName: string, userId: string): string {
  const timestamp = Date.now();
  const random = Math.random().toString(36).substring(7);
  const extension = originalName.split('.').pop();
  return `${timestamp}-${random}.${extension}`;
}

// =============================================================================
// SERVER-SIDE UTILITIES
// =============================================================================

export async function uploadFileServer(
  file: Buffer,
  fileName: string,
  bucket: BucketName,
  userId: string
): Promise<UploadResult> {
  const supabase = await createServerClient();

  const filePath = `${userId}/${fileName}`;

  const { data, error } = await supabase.storage.from(bucket).upload(filePath, file, {
    cacheControl: '3600',
    contentType: getContentType(fileName),
  });

  if (error) {
    throw new Error(`Failed to upload file: ${error.message}`);
  }

  let publicUrl: string | undefined;
  if (bucket !== 'proofs') {
    const { data: urlData } = supabase.storage.from(bucket).getPublicUrl(filePath);
    publicUrl = urlData.publicUrl;
  }

  return {
    path: data.path,
    fullPath: `${bucket}/${data.path}`,
    publicUrl,
  };
}

function getContentType(fileName: string): string {
  const extension = fileName.split('.').pop()?.toLowerCase();
  const mimeTypes: Record<string, string> = {
    pdf: 'application/pdf',
    jpg: 'image/jpeg',
    jpeg: 'image/jpeg',
    png: 'image/png',
    webp: 'image/webp',
    svg: 'image/svg+xml',
  };
  return mimeTypes[extension || ''] || 'application/octet-stream';
}
```

---

### 3.5 Step 4: Create Upload API Endpoint (30 minutes)

**File**: `/src/app/api/storage/upload/route.ts`

```typescript
import { NextRequest, NextResponse } from 'next/server';
import { createClient } from '@/lib/supabase/server';
import { uploadFileServer, BucketName, validateFile } from '@/lib/storage';
import { z } from 'zod';

const UploadSchema = z.object({
  bucket: z.enum(['proofs', 'avatars', 'covers', 'logos']),
  fileName: z.string().min(1),
});

export async function POST(request: NextRequest) {
  try {
    const supabase = await createClient();
    const {
      data: { user },
    } = await supabase.auth.getUser();

    if (!user) {
      return NextResponse.json({ error: 'Unauthorized' }, { status: 401 });
    }

    // Parse form data
    const formData = await request.formData();
    const file = formData.get('file') as File;
    const bucket = formData.get('bucket') as BucketName;

    if (!file) {
      return NextResponse.json({ error: 'No file provided' }, { status: 400 });
    }

    // Validate
    const validation = validateFile(file, bucket);
    if (!validation.valid) {
      return NextResponse.json({ error: validation.error }, { status: 400 });
    }

    // Convert File to Buffer
    const arrayBuffer = await file.arrayBuffer();
    const buffer = Buffer.from(arrayBuffer);

    // Upload
    const result = await uploadFileServer(buffer, file.name, bucket, user.id);

    return NextResponse.json({
      success: true,
      ...result,
    });
  } catch (error) {
    console.error('Upload error:', error);
    return NextResponse.json(
      { error: error instanceof Error ? error.message : 'Upload failed' },
      { status: 500 }
    );
  }
}
```

---

### 3.6 Step 5: Create Upload Component (1 hour)

**File**: `/src/components/storage/FileUpload.tsx`

```tsx
'use client';

import { useState, useRef } from 'react';
import { Button } from '@/components/ui/button';
import { Progress } from '@/components/ui/progress';
import { Alert, AlertDescription } from '@/components/ui/alert';
import { Upload, X, FileIcon, CheckCircle } from 'lucide-react';
import { uploadFile, BucketName, UploadResult } from '@/lib/storage';

interface FileUploadProps {
  bucket: BucketName;
  onUploadComplete?: (result: UploadResult) => void;
  maxSizeMB?: number;
  accept?: string;
  label?: string;
}

export function FileUpload({
  bucket,
  onUploadComplete,
  maxSizeMB = 5,
  accept,
  label = 'Upload File',
}: FileUploadProps) {
  const [file, setFile] = useState<File | null>(null);
  const [uploading, setUploading] = useState(false);
  const [progress, setProgress] = useState(0);
  const [error, setError] = useState<string | null>(null);
  const [success, setSuccess] = useState(false);
  const inputRef = useRef<HTMLInputElement>(null);

  const handleFileChange = (e: React.ChangeEvent<HTMLInputElement>) => {
    const selectedFile = e.target.files?.[0];
    if (selectedFile) {
      // Validate size
      if (selectedFile.size > maxSizeMB * 1024 * 1024) {
        setError(`File size exceeds ${maxSizeMB}MB limit`);
        return;
      }

      setFile(selectedFile);
      setError(null);
      setSuccess(false);
    }
  };

  const handleUpload = async () => {
    if (!file) return;

    setUploading(true);
    setError(null);
    setProgress(0);

    try {
      // Simulate progress (Supabase doesn't provide progress events)
      const progressInterval = setInterval(() => {
        setProgress((prev) => Math.min(prev + 10, 90));
      }, 200);

      const result = await uploadFile({
        bucket,
        file,
      });

      clearInterval(progressInterval);
      setProgress(100);
      setSuccess(true);

      if (onUploadComplete) {
        onUploadComplete(result);
      }

      // Reset after 2 seconds
      setTimeout(() => {
        setFile(null);
        setProgress(0);
        setSuccess(false);
        if (inputRef.current) {
          inputRef.current.value = '';
        }
      }, 2000);
    } catch (err) {
      setError(err instanceof Error ? err.message : 'Upload failed');
    } finally {
      setUploading(false);
    }
  };

  const handleRemove = () => {
    setFile(null);
    setError(null);
    setSuccess(false);
    setProgress(0);
    if (inputRef.current) {
      inputRef.current.value = '';
    }
  };

  return (
    <div className="space-y-4">
      {/* File Input */}
      <div className="flex items-center gap-3">
        <input
          ref={inputRef}
          type="file"
          onChange={handleFileChange}
          accept={accept}
          className="hidden"
          id="file-upload"
        />
        <label htmlFor="file-upload">
          <Button asChild variant="outline" disabled={uploading}>
            <span>
              <Upload className="mr-2 h-4 w-4" />
              {label}
            </span>
          </Button>
        </label>

        {file && (
          <div className="flex-1 flex items-center justify-between bg-gray-50 p-2 rounded">
            <div className="flex items-center gap-2">
              <FileIcon className="h-4 w-4 text-gray-500" />
              <span className="text-sm text-gray-700">{file.name}</span>
              <span className="text-xs text-gray-500">
                ({(file.size / 1024 / 1024).toFixed(2)} MB)
              </span>
            </div>
            <button onClick={handleRemove} disabled={uploading}>
              <X className="h-4 w-4 text-gray-500 hover:text-gray-700" />
            </button>
          </div>
        )}
      </div>

      {/* Upload Button */}
      {file && !success && (
        <Button onClick={handleUpload} disabled={uploading} className="w-full">
          {uploading ? 'Uploading...' : 'Upload'}
        </Button>
      )}

      {/* Progress */}
      {uploading && (
        <div className="space-y-2">
          <Progress value={progress} />
          <p className="text-xs text-center text-gray-500">{progress}%</p>
        </div>
      )}

      {/* Success */}
      {success && (
        <Alert className="bg-green-50 border-green-200">
          <CheckCircle className="h-4 w-4 text-green-600" />
          <AlertDescription className="text-green-800">
            File uploaded successfully!
          </AlertDescription>
        </Alert>
      )}

      {/* Error */}
      {error && (
        <Alert variant="destructive">
          <AlertDescription>{error}</AlertDescription>
        </Alert>
      )}
    </div>
  );
}
```

---

### 3.7 Step 6: Usage Example (15 minutes)

**Add to Profile Page**:

```tsx
// /src/app/app/i/profile/page.tsx

import { FileUpload } from '@/components/storage/FileUpload';

export default function ProfilePage() {
  const handleAvatarUpload = async (result: UploadResult) => {
    // Update profile with new avatar URL
    await fetch('/api/profile', {
      method: 'PATCH',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        avatarUrl: result.publicUrl,
      }),
    });
  };

  const handleProofUpload = async (result: UploadResult) => {
    // Add proof to evidence
    await fetch('/api/evidence', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        type: 'file',
        url: result.fullPath,
        name: 'Proof Document',
      }),
    });
  };

  return (
    <div>
      <h2>Profile Photo</h2>
      <FileUpload
        bucket="avatars"
        onUploadComplete={handleAvatarUpload}
        maxSizeMB={2}
        accept="image/*"
        label="Upload Avatar"
      />

      <h2>Add Proof Evidence</h2>
      <FileUpload
        bucket="proofs"
        onUploadComplete={handleProofUpload}
        maxSizeMB={5}
        accept="application/pdf,image/*"
        label="Upload Proof"
      />
    </div>
  );
}
```

---

### 3.8 Testing Checklist

- [ ] Upload avatar (‚â§2MB)
- [ ] Upload PDF proof (‚â§5MB)
- [ ] Verify file size validation works
- [ ] Verify MIME type validation works
- [ ] Confirm RLS: User can only access own files
- [ ] Test public URL for avatars
- [ ] Test signed URL for proofs
- [ ] Test file deletion

---

## 4. ANALYTICS TRACKING SYSTEM

### 4.1 Overview

**Goal**: Track all key events for North Star metrics

**Time Estimate**: 1 day

---

### 4.2 Step 1: Create Analytics Utility (30 minutes)

**File**: `/src/lib/analytics.ts`

```typescript
import { createClient } from '@/lib/supabase/server';

// =============================================================================
// EVENT TYPES (From PRD)
// =============================================================================

export type AnalyticsEvent =
  // Auth events
  | 'signed_up'
  | 'signed_in'
  | 'signed_out'
  // Profile events
  | 'created_profile'
  | 'profile_ready_for_match'
  | 'profile_updated'
  // Matching events
  | 'match_suggested'
  | 'match_viewed'
  | 'match_accepted'
  | 'match_declined'
  // Verification events
  | 'verification_requested'
  | 'verification_completed'
  | 'verification_appealed'
  // Messaging events
  | 'conversation_started'
  | 'conversation_revealed'
  | 'message_sent'
  // Assignment events
  | 'assignment_created'
  | 'assignment_published'
  | 'assignment_closed'
  // Org events
  | 'org_created'
  | 'org_verified'
  | 'org_member_invited'
  // Moderation events
  | 'content_reported'
  | 'moderation_action_taken';

// =============================================================================
// TRACK EVENT
// =============================================================================

export async function trackEvent(
  eventType: AnalyticsEvent,
  properties: Record<string, any> = {},
  userId?: string,
  orgId?: string
) {
  try {
    const supabase = await createClient();

    // Get user if not provided
    if (!userId) {
      const {
        data: { user },
      } = await supabase.auth.getUser();
      userId = user?.id;
    }

    // Insert event
    await supabase.from('analytics_events').insert({
      event_type: eventType,
      user_id: userId,
      org_id: orgId,
      properties,
      session_id: null, // TODO: Implement session tracking
      ip_address: null, // TODO: Implement IP tracking
      user_agent: null, // TODO: Implement user agent tracking
    });

    // Log to console in development
    if (process.env.NODE_ENV === 'development') {
      console.log(`üìä [Analytics] ${eventType}`, { userId, orgId, properties });
    }
  } catch (error) {
    // Don't throw - analytics failures shouldn't break app
    console.error('Failed to track event:', error);
  }
}

// =============================================================================
// CONVENIENCE FUNCTIONS
// =============================================================================

export async function trackSignUp(userId: string, method: 'email' | 'google' | 'linkedin') {
  await trackEvent('signed_up', { method }, userId);
}

export async function trackMatchAccepted(matchId: string, score: number, userId?: string) {
  await trackEvent('match_accepted', { matchId, score }, userId);
}

export async function trackMatchDeclined(
  matchId: string,
  reason?: string,
  userId?: string
) {
  await trackEvent('match_declined', { matchId, reason }, userId);
}

export async function trackVerificationCompleted(
  requestId: string,
  status: 'accepted' | 'declined' | 'cannot_verify',
  userId?: string
) {
  await trackEvent('verification_completed', { requestId, status }, userId);
}

export async function trackProfileReadyForMatch(userId: string) {
  await trackEvent('profile_ready_for_match', {}, userId);
}
```

---

### 4.3 Step 2: Add Tracking to Key Flows (2 hours)

#### Auth (Sign Up)

**File**: `/src/actions/auth.ts` (append)

```typescript
import { trackSignUp } from '@/lib/analytics';

// In signUpWithEmail function, after successful signup:
await trackSignUp(data.user.id, 'email');

// In OAuth callback handler:
if (isNewUser) {
  await trackSignUp(user.id, provider === 'google' ? 'google' : 'linkedin');
}
```

#### Matching (Accept)

**File**: `/src/app/api/matches/[id]/accept/route.ts`

```typescript
import { trackMatchAccepted } from '@/lib/analytics';

// After recording match interest:
await trackMatchAccepted(matchId, match.score, user.id);

// If mutual:
await trackEvent('conversation_started', { conversationId, matchId }, user.id);
```

#### Verification

**File**: `/src/app/api/verification/request/route.ts`

```typescript
import { trackEvent } from '@/lib/analytics';

// After creating verification request:
await trackEvent(
  'verification_requested',
  {
    requestId: verificationRequest.id,
    claimType: validated.claimType,
  },
  user.id
);
```

**File**: `/src/app/api/verification/respond/route.ts`

```typescript
// After verifier responds:
await trackVerificationCompleted(
  verificationRequest.id,
  validated.responseType,
  verificationRequest.profile_id
);
```

#### Messaging

**File**: `/src/app/api/conversations/[id]/messages/route.ts`

```typescript
// After sending message:
await trackEvent(
  'message_sent',
  {
    conversationId,
    messageId: message.id,
  },
  user.id
);
```

#### Assignment Creation

**File**: `/src/app/api/assignments/route.ts`

```typescript
// After creating assignment:
await trackEvent('assignment_created', { assignmentId: assignment.id }, user.id, orgId);

// When publishing:
await trackEvent('assignment_published', { assignmentId: assignment.id }, user.id, orgId);
```

---

### 4.4 Step 3: Create Metrics Dashboard (2 hours)

**File**: `/src/app/admin/metrics/page.tsx`

```tsx
import { createClient } from '@/lib/supabase/server';
import { Card, CardHeader, CardContent } from '@/components/ui/card';

// SQL functions for metrics
const METRICS_QUERIES = {
  timeToFirstMatch: `
    SELECT
      percentile_cont(0.5) WITHIN GROUP (
        ORDER BY EXTRACT(EPOCH FROM (mi.created_at - mp.created_at)) / 3600
      ) AS median_hours
    FROM match_interest mi
    JOIN matching_profiles mp ON mp.profile_id = mi.actor_profile_id
    WHERE mi.created_at = (
      SELECT MIN(created_at)
      FROM match_interest
      WHERE actor_profile_id = mi.actor_profile_id
    )
  `,

  profileCompletionRate: `
    SELECT
      COUNT(CASE WHEN ae.event_type = 'profile_ready_for_match'
                  AND ae.created_at < NOW() - INTERVAL '1 day' THEN 1 END)::float
        / NULLIF(COUNT(CASE WHEN ae.event_type = 'signed_up'
                             AND ae.created_at < NOW() - INTERVAL '1 day' THEN 1 END), 0) AS rate
    FROM analytics_events ae
  `,

  matchAcceptanceRate: `
    SELECT
      COUNT(CASE WHEN event_type = 'match_accepted' THEN 1 END)::float
        / NULLIF(COUNT(CASE WHEN event_type = 'match_viewed' THEN 1 END), 0) AS rate
    FROM analytics_events
    WHERE created_at > NOW() - INTERVAL '7 days'
  `,

  verifiedUsersRate: `
    SELECT
      COUNT(DISTINCT user_id)::float
        / NULLIF((SELECT COUNT(DISTINCT user_id)
                 FROM analytics_events
                 WHERE event_type = 'signed_up'
                   AND created_at > NOW() - INTERVAL '14 days'), 0) AS rate
    FROM analytics_events
    WHERE event_type = 'verification_completed'
      AND properties->>'status' = 'accepted'
      AND created_at > NOW() - INTERVAL '14 days'
  `,

  reportRate: `
    SELECT
      COUNT(*)::float
        / NULLIF((SELECT COUNT(*) FROM profiles), 0) AS rate
    FROM analytics_events
    WHERE event_type = 'content_reported'
      AND created_at > NOW() - INTERVAL '30 days'
  `,
};

export default async function MetricsDashboard() {
  const supabase = await createClient();

  // Fetch metrics
  const [
    timeToFirstMatch,
    profileCompletion,
    matchAcceptance,
    verifiedUsers,
    reportRate,
  ] = await Promise.all([
    supabase.rpc('metric_time_to_first_match'),
    supabase.rpc('metric_profile_completion_rate'),
    supabase.rpc('metric_match_acceptance_rate'),
    supabase.rpc('metric_verified_users_rate'),
    supabase.rpc('metric_report_rate'),
  ]);

  return (
    <div className="p-8 space-y-6">
      <h1 className="text-3xl font-bold">Metrics Dashboard</h1>

      <div className="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-4">
        {/* North Star Metric */}
        <MetricCard
          title="Time to First Match"
          value={`${timeToFirstMatch.data[0]?.median_hours?.toFixed(1) || 0}h`}
          target="<24h"
          description="Median time from profile creation to first accepted match"
        />

        <MetricCard
          title="Profile Completion D+1"
          value={`${((profileCompletion.data[0]?.rate || 0) * 100).toFixed(0)}%`}
          target="‚â•60%"
          description="% of users who complete profile within 24h of signup"
        />

        <MetricCard
          title="Match Acceptance Rate"
          value={`${((matchAcceptance.data[0]?.rate || 0) * 100).toFixed(0)}%`}
          target="‚â•20%"
          description="% of viewed matches that are accepted (7d rolling)"
        />

        <MetricCard
          title="Verified Users (14d)"
          value={`${((verifiedUsers.data[0]?.rate || 0) * 100).toFixed(0)}%`}
          target="‚â•30%"
          description="% of new users who get verified within 14 days"
        />

        <MetricCard
          title="Report Rate (30d)"
          value={`${((reportRate.data[0]?.rate || 0) * 100).toFixed(2)}%`}
          target="<1%"
          description="% of users who reported content in last 30 days"
        />
      </div>
    </div>
  );
}

function MetricCard({
  title,
  value,
  target,
  description,
}: {
  title: string;
  value: string;
  target: string;
  description: string;
}) {
  return (
    <Card>
      <CardHeader>
        <h3 className="text-sm font-medium text-gray-600">{title}</h3>
      </CardHeader>
      <CardContent>
        <div className="text-3xl font-bold">{value}</div>
        <p className="text-xs text-gray-500 mt-1">Target: {target}</p>
        <p className="text-xs text-gray-400 mt-2">{description}</p>
      </CardContent>
    </Card>
  );
}
```

---

### 4.5 Step 4: Create SQL Functions for Metrics (1 hour)

```sql
-- Run in Supabase SQL Editor

-- =============================================================================
-- TIME TO FIRST MATCH (North Star Metric #1)
-- =============================================================================

CREATE OR REPLACE FUNCTION metric_time_to_first_match()
RETURNS TABLE (median_hours float)
LANGUAGE sql
STABLE
AS $$
  SELECT
    percentile_cont(0.5) WITHIN GROUP (
      ORDER BY EXTRACT(EPOCH FROM (mi.created_at - mp.created_at)) / 3600
    ) AS median_hours
  FROM match_interest mi
  JOIN matching_profiles mp ON mp.profile_id = mi.actor_profile_id
  WHERE mi.created_at = (
    SELECT MIN(created_at)
    FROM match_interest
    WHERE actor_profile_id = mi.actor_profile_id
  );
$$;

-- =============================================================================
-- PROFILE COMPLETION RATE
-- =============================================================================

CREATE OR REPLACE FUNCTION metric_profile_completion_rate()
RETURNS TABLE (rate float)
LANGUAGE sql
STABLE
AS $$
  SELECT
    COUNT(CASE WHEN ae.event_type = 'profile_ready_for_match'
                AND ae.created_at < NOW() - INTERVAL '1 day' THEN 1 END)::float
      / NULLIF(COUNT(CASE WHEN ae.event_type = 'signed_up'
                           AND ae.created_at < NOW() - INTERVAL '1 day' THEN 1 END), 0) AS rate
  FROM analytics_events ae;
$$;

-- =============================================================================
-- MATCH ACCEPTANCE RATE
-- =============================================================================

CREATE OR REPLACE FUNCTION metric_match_acceptance_rate()
RETURNS TABLE (rate float)
LANGUAGE sql
STABLE
AS $$
  SELECT
    COUNT(CASE WHEN event_type = 'match_accepted' THEN 1 END)::float
      / NULLIF(COUNT(CASE WHEN event_type = 'match_viewed' THEN 1 END), 0) AS rate
  FROM analytics_events
  WHERE created_at > NOW() - INTERVAL '7 days';
$$;

-- =============================================================================
-- VERIFIED USERS RATE (14 days)
-- =============================================================================

CREATE OR REPLACE FUNCTION metric_verified_users_rate()
RETURNS TABLE (rate float)
LANGUAGE sql
STABLE
AS $$
  SELECT
    COUNT(DISTINCT user_id)::float
      / NULLIF((SELECT COUNT(DISTINCT user_id)
               FROM analytics_events
               WHERE event_type = 'signed_up'
                 AND created_at > NOW() - INTERVAL '14 days'), 0) AS rate
  FROM analytics_events
  WHERE event_type = 'verification_completed'
    AND properties->>'status' = 'accepted'
    AND created_at > NOW() - INTERVAL '14 days';
$$;

-- =============================================================================
-- REPORT RATE (30 days)
-- =============================================================================

CREATE OR REPLACE FUNCTION metric_report_rate()
RETURNS TABLE (rate float)
LANGUAGE sql
STABLE
AS $$
  SELECT
    COUNT(*)::float
      / NULLIF((SELECT COUNT(*) FROM profiles), 0) AS rate
  FROM analytics_events
  WHERE event_type = 'content_reported'
    AND created_at > NOW() - INTERVAL '30 days';
$$;
```

---

### 4.6 Testing Checklist

- [ ] Sign up ‚Üí verify `signed_up` event tracked
- [ ] Complete profile ‚Üí verify `profile_ready_for_match` tracked
- [ ] Accept match ‚Üí verify `match_accepted` tracked
- [ ] Send message ‚Üí verify `message_sent` tracked
- [ ] View metrics dashboard ‚Üí verify SQL functions return data
- [ ] Check `analytics_events` table has events

---

## 5. SKILLS TAXONOMY EXPANSION

### 5.1 Goal

Expand from 114 skills to 500+ core skills.

**Time Estimate**: 4-6 hours

---

### 5.2 Additional Skills (386 more)

**File**: `/src/lib/taxonomy/data.ts` (append to `SKILLS_TAXONOMY`)

```typescript
// ... (existing 114 skills)

// =============================================================================
// ENGINEERING (50 more) - Total: ~70
// =============================================================================
{ key: 'rust', label: 'Rust', category: 'Engineering' },
{ key: 'go', label: 'Go', category: 'Engineering' },
{ key: 'kotlin', label: 'Kotlin', category: 'Engineering' },
{ key: 'swift', label: 'Swift', category: 'Engineering' },
{ key: 'php', label: 'PHP', category: 'Engineering' },
{ key: 'ruby', label: 'Ruby', category: 'Engineering' },
{ key: 'c-sharp', label: 'C#', category: 'Engineering' },
{ key: 'c-plus-plus', label: 'C++', category: 'Engineering' },
{ key: 'java', label: 'Java', category: 'Engineering' },
{ key: 'scala', label: 'Scala', category: 'Engineering' },
{ key: 'elixir', label: 'Elixir', category: 'Engineering' },
{ key: 'clojure', label: 'Clojure', category: 'Engineering' },

// Frontend Frameworks
{ key: 'vue', label: 'Vue.js', category: 'Engineering' },
{ key: 'angular', label: 'Angular', category: 'Engineering' },
{ key: 'svelte', label: 'Svelte', category: 'Engineering' },
{ key: 'next-js', label: 'Next.js', category: 'Engineering' },
{ key: 'nuxt', label: 'Nuxt.js', category: 'Engineering' },
{ key: 'gatsby', label: 'Gatsby', category: 'Engineering' },
{ key: 'remix', label: 'Remix', category: 'Engineering' },

// Backend Frameworks
{ key: 'django', label: 'Django', category: 'Engineering' },
{ key: 'flask', label: 'Flask', category: 'Engineering' },
{ key: 'fastapi', label: 'FastAPI', category: 'Engineering' },
{ key: 'spring-boot', label: 'Spring Boot', category: 'Engineering' },
{ key: 'express', label: 'Express.js', category: 'Engineering' },
{ key: 'nestjs', label: 'NestJS', category: 'Engineering' },
{ key: 'laravel', label: 'Laravel', category: 'Engineering' },
{ key: 'rails', label: 'Ruby on Rails', category: 'Engineering' },
{ key: 'asp-net', label: 'ASP.NET', category: 'Engineering' },

// APIs
{ key: 'graphql', label: 'GraphQL', category: 'Engineering' },
{ key: 'rest-api', label: 'REST APIs', category: 'Engineering' },
{ key: 'grpc', label: 'gRPC', category: 'Engineering' },
{ key: 'websockets', label: 'WebSockets', category: 'Engineering' },
{ key: 'api-design', label: 'API Design', category: 'Engineering' },

// DevOps & Cloud
{ key: 'docker', label: 'Docker', category: 'Engineering' },
{ key: 'kubernetes', label: 'Kubernetes', category: 'Engineering' },
{ key: 'terraform', label: 'Terraform', category: 'Engineering' },
{ key: 'ansible', label: 'Ansible', category: 'Engineering' },
{ key: 'jenkins', label: 'Jenkins', category: 'Engineering' },
{ key: 'github-actions', label: 'GitHub Actions', category: 'Engineering' },
{ key: 'gitlab-ci', label: 'GitLab CI/CD', category: 'Engineering' },
{ key: 'circleci', label: 'CircleCI', category: 'Engineering' },
{ key: 'aws', label: 'AWS', category: 'Engineering' },
{ key: 'azure', label: 'Microsoft Azure', category: 'Engineering' },
{ key: 'gcp', label: 'Google Cloud Platform', category: 'Engineering' },
{ key: 'heroku', label: 'Heroku', category: 'Engineering' },
{ key: 'vercel', label: 'Vercel', category: 'Engineering' },
{ key: 'netlify', label: 'Netlify', category: 'Engineering' },

// Databases
{ key: 'postgresql', label: 'PostgreSQL', category: 'Engineering' },
{ key: 'mysql', label: 'MySQL', category: 'Engineering' },
{ key: 'mongodb', label: 'MongoDB', category: 'Engineering' },
{ key: 'redis', label: 'Redis', category: 'Engineering' },
{ key: 'cassandra', label: 'Cassandra', category: 'Engineering' },
{ key: 'dynamodb', label: 'DynamoDB', category: 'Engineering' },
{ key: 'elasticsearch', label: 'Elasticsearch', category: 'Engineering' },
{ key: 'firebase', label: 'Firebase', category: 'Engineering' },
{ key: 'supabase', label: 'Supabase', category: 'Engineering' },

// Mobile
{ key: 'react-native', label: 'React Native', category: 'Engineering' },
{ key: 'flutter', label: 'Flutter', category: 'Engineering' },
{ key: 'ios-development', label: 'iOS Development', category: 'Engineering' },
{ key: 'android-development', label: 'Android Development', category: 'Engineering' },
{ key: 'swift-ui', label: 'SwiftUI', category: 'Engineering' },
{ key: 'jetpack-compose', label: 'Jetpack Compose', category: 'Engineering' },

// Tools & Others
{ key: 'git', label: 'Git & Version Control', category: 'Engineering' },
{ key: 'linux', label: 'Linux', category: 'Engineering' },
{ key: 'bash', label: 'Bash Scripting', category: 'Engineering' },
{ key: 'testing', label: 'Testing & QA', category: 'Engineering' },
{ key: 'cypress', label: 'Cypress', category: 'Engineering' },
{ key: 'jest', label: 'Jest', category: 'Engineering' },
{ key: 'playwright', label: 'Playwright', category: 'Engineering' },
{ key: 'web-security', label: 'Web Security', category: 'Engineering' },
{ key: 'oauth', label: 'OAuth/OpenID Connect', category: 'Engineering' },
{ key: 'jwt', label: 'JWT', category: 'Engineering' },
{ key: 'accessibility', label: 'Web Accessibility (WCAG)', category: 'Engineering' },
{ key: 'performance', label: 'Performance Optimization', category: 'Engineering' },
{ key: 'seo', label: 'SEO', category: 'Engineering' },
{ key: 'i18n', label: 'Internationalization (i18n)', category: 'Engineering' },

// =============================================================================
// DATA & AI (60 more) - Total: ~70
// =============================================================================

// Languages & Libraries
{ key: 'python-data', label: 'Python (Data Science)', category: 'Data & AI' },
{ key: 'r', label: 'R', category: 'Data & AI' },
{ key: 'julia', label: 'Julia', category: 'Data & AI' },
{ key: 'pandas', label: 'Pandas', category: 'Data & AI' },
{ key: 'numpy', label: 'NumPy', category: 'Data & AI' },
{ key: 'scipy', label: 'SciPy', category: 'Data & AI' },
{ key: 'matplotlib', label: 'Matplotlib', category: 'Data & AI' },
{ key: 'seaborn', label: 'Seaborn', category: 'Data & AI' },
{ key: 'plotly', label: 'Plotly', category: 'Data & AI' },

// Machine Learning
{ key: 'scikit-learn', label: 'Scikit-learn', category: 'Data & AI' },
{ key: 'tensorflow', label: 'TensorFlow', category: 'Data & AI' },
{ key: 'pytorch', label: 'PyTorch', category: 'Data & AI' },
{ key: 'keras', label: 'Keras', category: 'Data & AI' },
{ key: 'xgboost', label: 'XGBoost', category: 'Data & AI' },
{ key: 'lightgbm', label: 'LightGBM', category: 'Data & AI' },
{ key: 'catboost', label: 'CatBoost', category: 'Data & AI' },

// Deep Learning & AI
{ key: 'deep-learning', label: 'Deep Learning', category: 'Data & AI' },
{ key: 'neural-networks', label: 'Neural Networks', category: 'Data & AI' },
{ key: 'cnn', label: 'Convolutional Neural Networks (CNN)', category: 'Data & AI' },
{ key: 'rnn', label: 'Recurrent Neural Networks (RNN)', category: 'Data & AI' },
{ key: 'transformer', label: 'Transformer Models', category: 'Data & AI' },
{ key: 'nlp', label: 'Natural Language Processing', category: 'Data & AI' },
{ key: 'computer-vision', label: 'Computer Vision', category: 'Data & AI' },
{ key: 'reinforcement-learning', label: 'Reinforcement Learning', category: 'Data & AI' },
{ key: 'gans', label: 'Generative Adversarial Networks (GANs)', category: 'Data & AI' },

// LLMs & Generative AI
{ key: 'llm', label: 'Large Language Models', category: 'Data & AI' },
{ key: 'prompt-engineering', label: 'Prompt Engineering', category: 'Data & AI' },
{ key: 'openai-api', label: 'OpenAI API', category: 'Data & AI' },
{ key: 'langchain', label: 'LangChain', category: 'Data & AI' },
{ key: 'llamaindex', label: 'LlamaIndex', category: 'Data & AI' },
{ key: 'huggingface', label: 'Hugging Face', category: 'Data & AI' },
{ key: 'fine-tuning', label: 'Model Fine-tuning', category: 'Data & AI' },
{ key: 'rag', label: 'Retrieval Augmented Generation (RAG)', category: 'Data & AI' },

// Vector & Embeddings
{ key: 'vector-databases', label: 'Vector Databases', category: 'Data & AI' },
{ key: 'embeddings', label: 'Embeddings', category: 'Data & AI' },
{ key: 'pinecone', label: 'Pinecone', category: 'Data & AI' },
{ key: 'weaviate', label: 'Weaviate', category: 'Data & AI' },
{ key: 'chromadb', label: 'ChromaDB', category: 'Data & AI' },

// Data Engineering
{ key: 'data-engineering', label: 'Data Engineering', category: 'Data & AI' },
{ key: 'etl', label: 'ETL Pipelines', category: 'Data & AI' },
{ key: 'airflow', label: 'Apache Airflow', category: 'Data & AI' },
{ key: 'spark', label: 'Apache Spark', category: 'Data & AI' },
{ key: 'hadoop', label: 'Hadoop', category: 'Data & AI' },
{ key: 'kafka', label: 'Apache Kafka', category: 'Data & AI' },
{ key: 'dbt', label: 'dbt (Data Build Tool)', category: 'Data & AI' },
{ key: 'dagster', label: 'Dagster', category: 'Data & AI' },

// Data Warehousing & BI
{ key: 'data-warehousing', label: 'Data Warehousing', category: 'Data & AI' },
{ key: 'snowflake', label: 'Snowflake', category: 'Data & AI' },
{ key: 'bigquery', label: 'BigQuery', category: 'Data & AI' },
{ key: 'redshift', label: 'Amazon Redshift', category: 'Data & AI' },
{ key: 'tableau', label: 'Tableau', category: 'Data & AI' },
{ key: 'power-bi', label: 'Power BI', category: 'Data & AI' },
{ key: 'looker', label: 'Looker', category: 'Data & AI' },
{ key: 'metabase', label: 'Metabase', category: 'Data & AI' },

// Statistics & Analysis
{ key: 'statistics', label: 'Statistics', category: 'Data & AI' },
{ key: 'ab-testing', label: 'A/B Testing', category: 'Data & AI' },
{ key: 'causal-inference', label: 'Causal Inference', category: 'Data & AI' },
{ key: 'time-series', label: 'Time Series Analysis', category: 'Data & AI' },
{ key: 'forecasting', label: 'Forecasting', category: 'Data & AI' },
{ key: 'clustering', label: 'Clustering', category: 'Data & AI' },
{ key: 'classification', label: 'Classification', category: 'Data & AI' },
{ key: 'regression', label: 'Regression Analysis', category: 'Data & AI' },
{ key: 'dimensionality-reduction', label: 'Dimensionality Reduction', category: 'Data & AI' },
{ key: 'feature-engineering', label: 'Feature Engineering', category: 'Data & AI' },

// MLOps
{ key: 'mlops', label: 'MLOps', category: 'Data & AI' },
{ key: 'model-deployment', label: 'ML Model Deployment', category: 'Data & AI' },
{ key: 'model-monitoring', label: 'Model Monitoring', category: 'Data & AI' },
{ key: 'mlflow', label: 'MLflow', category: 'Data & AI' },
{ key: 'kubeflow', label: 'Kubeflow', category: 'Data & AI' },

// ... Continue with remaining categories: Design, Product, Marketing, Operations, Leadership
// Full 500 skills available on request to avoid character limit
```

*(Due to length constraints, the full 500 skills list can be provided separately. The pattern above shows the approach.)*

---

## 6. RATE LIMITING IMPLEMENTATION

### 6.1 Overview

**Goal**: Prevent abuse with rate limiting per IP and per user

**PRD Requirements**: 60 req/min per IP, 120 req/min per user

**Time Estimate**: 3 hours

---

### 6.2 Step 1: Install Dependencies (5 minutes)

```bash
npm install @upstash/ratelimit @upstash/redis
```

Or use built-in Supabase:

```bash
# No install needed, use existing database
```

---

### 6.3 Step 2: Create Rate Limit Utility (1 hour)

**File**: `/src/lib/rate-limit.ts`

```typescript
import { createClient } from '@/lib/supabase/admin';

// =============================================================================
// RATE LIMIT CONFIGURATION
// =============================================================================

export interface RateLimitConfig {
  identifier: string; // IP address or user ID
  max: number; // Max requests
  window: number; // Window in seconds
}

const RATE_LIMITS = {
  ip: {
    max: 60,
    window: 60, // 60 requests per 60 seconds
  },
  user: {
    max: 120,
    window: 60, // 120 requests per 60 seconds
  },
  auth: {
    max: 10,
    window: 300, // 10 requests per 5 minutes (stricter for auth)
  },
  verification: {
    max: 5,
    window: 3600, // 5 requests per hour
  },
};

// =============================================================================
// CHECK RATE LIMIT (Using Supabase Database)
// =============================================================================

export async function checkRateLimit(
  identifier: string,
  limitType: keyof typeof RATE_LIMITS = 'ip'
): Promise<{ allowed: boolean; remaining: number; resetAt: Date }> {
  const supabase = createClient();
  const config = RATE_LIMITS[limitType];
  const now = new Date();
  const windowStart = new Date(now.getTime() - config.window * 1000);

  // Get or create rate limit record
  const { data: existing } = await supabase
    .from('rate_limits')
    .select('*')
    .eq('identifier', identifier)
    .eq('limit_type', limitType)
    .single();

  if (!existing || new Date(existing.reset_at) < now) {
    // Create new window
    await supabase
      .from('rate_limits')
      .upsert({
        identifier,
        limit_type: limitType,
        count: 1,
        reset_at: new Date(now.getTime() + config.window * 1000).toISOString(),
        created_at: now.toISOString(),
      }, {
        onConflict: 'identifier,limit_type',
      });

    return {
      allowed: true,
      remaining: config.max - 1,
      resetAt: new Date(now.getTime() + config.window * 1000),
    };
  }

  // Check if within limit
  if (existing.count >= config.max) {
    return {
      allowed: false,
      remaining: 0,
      resetAt: new Date(existing.reset_at),
    };
  }

  // Increment count
  await supabase
    .from('rate_limits')
    .update({ count: existing.count + 1 })
    .eq('identifier', identifier)
    .eq('limit_type', limitType);

  return {
    allowed: true,
    remaining: config.max - (existing.count + 1),
    resetAt: new Date(existing.reset_at),
  };
}

// =============================================================================
// GET CLIENT IP
// =============================================================================

export function getClientIp(request: Request): string {
  // Check headers in order of preference
  const forwarded = request.headers.get('x-forwarded-for');
  if (forwarded) {
    return forwarded.split(',')[0].trim();
  }

  const realIp = request.headers.get('x-real-ip');
  if (realIp) {
    return realIp;
  }

  const cfConnectingIp = request.headers.get('cf-connecting-ip');
  if (cfConnectingIp) {
    return cfConnectingIp;
  }

  return 'unknown';
}
```

---

### 6.4 Step 3: Add Middleware (1 hour)

**File**: `/src/middleware.ts` (update)

```typescript
import { NextResponse } from 'next/server';
import type { NextRequest } from 'next/server';
import { checkRateLimit, getClientIp } from '@/lib/rate-limit';
import { createClient } from '@/lib/supabase/server';

export async function middleware(request: NextRequest) {
  const path = request.nextUrl.pathname;

  // Skip rate limiting for static files
  if (
    path.startsWith('/_next') ||
    path.startsWith('/static') ||
    path.includes('.')
  ) {
    return NextResponse.next();
  }

  // Apply rate limiting to API routes
  if (path.startsWith('/api')) {
    const ip = getClientIp(request);

    // IP rate limit (60/min)
    const ipLimit = await checkRateLimit(ip, 'ip');
    if (!ipLimit.allowed) {
      return NextResponse.json(
        {
          error: 'Too many requests from this IP',
          resetAt: ipLimit.resetAt.toISOString(),
        },
        {
          status: 429,
          headers: {
            'X-RateLimit-Limit': '60',
            'X-RateLimit-Remaining': '0',
            'X-RateLimit-Reset': ipLimit.resetAt.getTime().toString(),
            'Retry-After': Math.ceil(
              (ipLimit.resetAt.getTime() - Date.now()) / 1000
            ).toString(),
          },
        }
      );
    }

    // User rate limit (120/min) for authenticated requests
    const supabase = await createClient();
    const {
      data: { user },
    } = await supabase.auth.getUser();

    if (user) {
      const userLimit = await checkRateLimit(user.id, 'user');
      if (!userLimit.allowed) {
        return NextResponse.json(
          {
            error: 'Too many requests from this user',
            resetAt: userLimit.resetAt.toISOString(),
          },
          {
            status: 429,
            headers: {
              'X-RateLimit-Limit': '120',
              'X-RateLimit-Remaining': '0',
              'X-RateLimit-Reset': userLimit.resetAt.getTime().toString(),
            },
          }
        );
      }
    }

    // Stricter limit for auth endpoints
    if (path.includes('/api/auth') || path.includes('/api/verification')) {
      const authLimit = await checkRateLimit(ip, 'auth');
      if (!authLimit.allowed) {
        return NextResponse.json(
          { error: 'Too many authentication attempts' },
          { status: 429 }
        );
      }
    }
  }

  return NextResponse.next();
}

export const config = {
  matcher: [
    /*
     * Match all request paths except:
     * - _next/static (static files)
     * - _next/image (image optimization files)
     * - favicon.ico (favicon file)
     * - public folder
     */
    '/((?!_next/static|_next/image|favicon.ico|.*\\.(?:svg|png|jpg|jpeg|gif|webp)$).*)',
  ],
};
```

---

### 6.5 Testing Checklist

- [ ] Make 61 requests to API ‚Üí verify 429 error
- [ ] Wait 1 minute ‚Üí verify requests work again
- [ ] Check rate limit headers in response
- [ ] Test auth endpoint rate limit (10 req in 5 min)
- [ ] Verify user rate limit (120/min) works

---

## 7. SECURITY HARDENING

### 7.1 Content Security Policy (30 minutes)

**File**: `/next.config.js` (update)

```javascript
/** @type {import('next').NextConfig} */
const nextConfig = {
  // ... existing config

  async headers() {
    return [
      {
        source: '/(.*)',
        headers: [
          {
            key: 'Content-Security-Policy',
            value: [
              "default-src 'self'",
              "script-src 'self' 'unsafe-eval' 'unsafe-inline' https://vercel.live https://cdn.vercel-insights.com",
              "style-src 'self' 'unsafe-inline'",
              "img-src 'self' data: https: blob:",
              "font-src 'self' data:",
              "connect-src 'self' https://*.supabase.co wss://*.supabase.co https://vercel.live",
              "frame-src 'self' https://vercel.live",
              "object-src 'none'",
              "base-uri 'self'",
              "form-action 'self'",
              "frame-ancestors 'none'",
              "upgrade-insecure-requests",
            ]
              .join('; ')
              .replace(/\\n/g, ''),
          },
          {
            key: 'X-Frame-Options',
            value: 'DENY',
          },
          {
            key: 'X-Content-Type-Options',
            value: 'nosniff',
          },
          {
            key: 'Referrer-Policy',
            value: 'strict-origin-when-cross-origin',
          },
          {
            key: 'Permissions-Policy',
            value: 'camera=(), microphone=(), geolocation=()',
          },
        ],
      },
    ];
  },
};

module.exports = nextConfig;
```

---

### 7.2 Input Sanitization (1 hour)

Install DOMPurify:

```bash
npm install dompurify @types/dompurify
```

**File**: `/src/lib/sanitize.ts`

```typescript
import DOMPurify from 'dompurify';

export function sanitizeHtml(html: string): string {
  return DOMPurify.sanitize(html, {
    ALLOWED_TAGS: ['b', 'i', 'em', 'strong', 'a', 'p', 'br', 'ul', 'ol', 'li'],
    ALLOWED_ATTR: ['href', 'target', 'rel'],
  });
}

export function sanitizeText(text: string): string {
  return text.replace(/<[^>]*>/g, ''); // Strip all HTML tags
}
```

**Usage**:

```typescript
// In any API endpoint that accepts user input:
import { sanitizeText, sanitizeHtml } from '@/lib/sanitize';

const cleanBio = sanitizeHtml(body.bio);
const cleanMessage = sanitizeText(body.message);
```

---

### 7.3 CSRF Protection (Already Handled)

Supabase Auth includes CSRF protection by default. No additional work needed.

---

### 7.4 Testing Checklist

- [ ] Try to inject `<script>alert('XSS')</script>` in bio ‚Üí verify stripped
- [ ] Check CSP headers in browser DevTools
- [ ] Verify iframe embedding fails
- [ ] Test CORS (should only allow own domain)

---

## 8. TESTING CHECKLISTS

### 8.1 File Storage

- [ ] Upload avatar (‚â§2MB) ‚Üí Success
- [ ] Upload oversized file (>5MB) ‚Üí Error
- [ ] Upload wrong file type ‚Üí Error
- [ ] RLS: User A cannot access User B's files
- [ ] Get public URL for avatar
- [ ] Get signed URL for proof
- [ ] Delete file ‚Üí Success

### 8.2 Analytics

- [ ] Sign up ‚Üí Check `analytics_events` table
- [ ] Accept match ‚Üí Check event tracked
- [ ] Send message ‚Üí Check event tracked
- [ ] View metrics dashboard ‚Üí Data displayed
- [ ] SQL functions return correct values

### 8.3 Skills Taxonomy

- [ ] Search for "React" ‚Üí Found
- [ ] Search for "Python" ‚Üí Found
- [ ] Count skills ‚Üí ‚â•500
- [ ] UI autocomplete works

### 8.4 Rate Limiting

- [ ] 61 API requests ‚Üí 429 error
- [ ] Wait 1 minute ‚Üí Success
- [ ] Check `X-RateLimit-*` headers
- [ ] Auth endpoint: 11 attempts in 5 min ‚Üí 429

### 8.5 Security

- [ ] Inject XSS ‚Üí Stripped
- [ ] Check CSP headers
- [ ] Try iframe embed ‚Üí Blocked
- [ ] SQL injection attempt ‚Üí Blocked by Drizzle

---

## CONCLUSION

This guide provides step-by-step instructions for implementing the most critical missing systems. Each section is designed to be actionable with copy-paste code examples.

**Total Time Estimate**: 3-5 days (1 developer)

**Priority Order**:
1. File Storage (Day 1)
2. Analytics Tracking (Day 2)
3. Rate Limiting (Day 3)
4. Security Hardening (Day 3)
5. Skills Taxonomy Expansion (Day 4-5, can be done in parallel)

**Next Steps**:
1. Implement these systems in parallel with Verification & Messaging
2. Test thoroughly
3. Deploy to staging
4. Launch MVP!

---

*For the complete MVP plan, see `MVP_IMPLEMENTATION_PLAN.md`*

**Document Status**: ‚úÖ Complete
**Last Updated**: 2025-10-30
